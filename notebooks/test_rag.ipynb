{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "836f5d48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“š **Relevant excerpts from course materials:**\n",
      "\n",
      "**From 2_DL_Adv_Deep_Learning.txt:**\n",
      "Students gain a deeper understanding of the DL learning process, by grasping\n",
      "The role of the learning rate and activation functions\n",
      "The difficulties of complex loss landscapes, and how to mitigates them\n",
      "The role of regularization techniques (early stopping, weight decay, dropout, ensemble models, data augmentation, batch normalization)\n",
      "\n",
      "Students can explain the purpose, and core architecture, of several different major neural network types\n",
      "\n",
      "10\n",
      "Neural Networks\n",
      "AI > ML > DL\n",
      "11\n",
      "\n",
      "---\n",
      "\n",
      "**From 3_DL_ComputerVision_Classification.txt:**\n",
      "Enter deep learning = many hidden layers.\n",
      "\n",
      "Made feasible by GPUs and better techniques.\n",
      "\n",
      "Finding the optimal weights\n",
      "5\n",
      "Forward pass (calculate error w.r.t ground truth)\n",
      "Backwards pass (update weights to decrease error)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Repeat until no \n",
      "further improvement\n",
      "Problem 1: Local Minima (in loss landscape)\n",
      "6\n",
      "Forward pass (calculate error w.r.t ground truth)\n",
      "Backwards pass (calculate Dw that decreases error)\n",
      "\n",
      "---\n",
      "\n",
      "**From 2_DL_Adv_Deep_Learning.txt:**\n",
      "Layers of a Deep Learning Network\n",
      "27\n",
      "\n",
      "Learning Cycle\n",
      "28\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Step1 Forward Pass\n",
      "Step2 Loss Function\n",
      "Step3 Optimize weights\n",
      "Step4 Back-propagate\n",
      "Epoch â€“ Batch - Iteration\n",
      "29\n",
      "\n",
      "Underfit â€“ Overfit: use of validation set\n",
      "30\n",
      "\n",
      "After training: inference (predict)\n",
      "31\n",
      "\n",
      "32\n",
      "Advanced Topics\n",
      "33\n",
      "Data Preprocessing\n",
      "Data Preprocessing\n",
      "34\n",
      "\n",
      "Normalize - Standardize\n",
      "35\n",
      "\n",
      "36\n",
      "Forward Pass & Loss calculation & Backward pass\n",
      "Loss calculation\n",
      "37\n",
      "\n",
      "Backwards Pass\n",
      "38\n",
      "\n",
      "Update Weights (simplification)\n",
      "39\n",
      "\n",
      "Learning Rate\n",
      "40\n",
      "\n",
      "---\n",
      "\n",
      "**From 3_DL_ComputerVision_Classification.txt:**\n",
      "Inception module\u000b(optimized via 1x1 bottleneck)\n",
      "70\n",
      "\n",
      "4. ResNet (the go-to option!)\u000b\t\t\t\t\t\t- 2015\n",
      "71\n",
      "Innovation: \n",
      "Skip connections (shortcuts)\n",
      "Residual learning forces the model to learn the â€˜restâ€™, what other the other layers didnâ€™t (the details)\n",
      "Skip connections create shortcuts for easier backpropagation of the gradient, so we can go deeper (more layers)\n",
      "Side by side comparison\n",
      "72\n",
      "\n",
      "5. DenseNet - 2016\u000b(skip-connections overkill?)\n",
      "73\n",
      "\n",
      "---\n",
      "\n",
      "**From 2_DL_Adv_Deep_Learning.txt:**\n",
      "Backwards Pass\n",
      "38\n",
      "\n",
      "Update Weights (simplification)\n",
      "39\n",
      "\n",
      "Learning Rate\n",
      "40\n",
      "\n",
      "41\n",
      "Challenge\n",
      "\n",
      "Local Minima\n",
      "\n",
      "â€˜Iâ€™m stuck!â€™\n",
      "Getting Stuck in Local Minima\u000b(of the loss function)\n",
      "42\n",
      "\n",
      "Loss Landscapes: theory versus reality\n",
      "43\n",
      "Local Minima & Saddle Points\n",
      "44\n",
      "\n",
      "Solution 1:\u000bRandomness by Batch Size\n",
      "45\n",
      "\n",
      "Solution 2:\u000bRandomness by Learning Rate\n",
      "46\n",
      "Annealing Cycling LR\n",
      "\n",
      "\t\t\tOne Cycle ïƒ \n",
      "One Cycle Learning Rate Scheme\n",
      "47\n",
      "\n",
      "Solution 3:\u000bMomentum Optimizer (adam)\n",
      "48\n",
      "\n",
      "49\n",
      "Challenge\n",
      "\n",
      "\t\t\n",
      "\n",
      "\t\t\tVanishing Gradients\n",
      "\n",
      "---\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from backend.tools_rag import course_docs_search\n",
    "\n",
    "result = course_docs_search.invoke(\"What is gradient descent?\")\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
